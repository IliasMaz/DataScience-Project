# Customer Segmentation via Clustering: A Data-Driven Approach

## Overview

This project delves into customer transaction data to uncover distinct segments through unsupervised machine learning. By applying clustering techniques, we aim to transform raw data into a customer-centric dataset, offering valuable insights into customer behaviors. These insights can then empower targeted marketing strategies and enhance data-driven decision-making processes.

The workflow is structured in two primary phases:
1.  **Data Acquisition:** Fetching initial data using the `API.py` script.
2.  **Analysis & Modeling:** In-depth data cleaning, exploratory data analysis (EDA), feature engineering, and K-Means clustering performed within the `project.ipynb` Jupyter Notebook.

---

## 1. Data Acquisition: `API.py`

The first step in our pipeline involves programmatic data retrieval.

* **Purpose:** The `API.py` script is designed to fetch the foundational dataset from a designated REST API.
* **Core Functionality:**
    * Executes a GET request to the API endpoint: `http://egov.dai.uom.gr:5001/data`.
    * Parses the incoming JSON response.
    * Transforms the JSON data into a structured pandas DataFrame.
    * Persists the DataFrame locally as `dataset.csv`.

---

## 2. Customer Clustering Analysis: `project.ipynb`

This Jupyter Notebook is where the core data transformation and modeling take place.

### Objective

The central goal is to leverage K-Means clustering, an unsupervised learning algorithm, to identify and group customers into meaningful segments. This segmentation provides a deeper understanding of customer archetypes and behaviors.

### Dataset

The analysis utilizes a CSV file named `Dataset (1).csv`.

‚ö†Ô∏è **Important Note for Reproducibility:** The `API.py` script outputs `dataset.csv`. The notebook currently uses an absolute path: `pd.read_csv('/Users/liakooras/Desktop/Msc/CAPSTONE/Dataset (1).csv')`. To ensure the project runs correctly in other environments, it is highly recommended to:
    1. Modify the notebook to load `dataset.csv` using a relative path (e.g., `dataset = pd.read_csv('dataset.csv')`).
    2. Ensure `dataset.csv` (generated by `API.py`) is the intended input for the notebook, or place your specific `Dataset (1).csv` in the project's root directory and update the path accordingly.

The dataset initially comprises the following features: `Transaction ID`, `Date`, `Customer ID`, `Gender`, `Age`, `Product Category`, `Quantity`, `Price per Unit`, and `Total Amount`.

### Data Cleaning & Preprocessing

A series of data cleaning and preprocessing steps were meticulously applied:

* **Data Type Conversion:**
    * The `Date` column was converted from an `object` type to a `datetime` object for time-series analysis.
* **Handling Missing Values (NaNs):**
    * `Age`: Missing values were imputed using the mode (43.0). Age values below 16 (considered outliers or invalid entries) were adjusted to 16.
    * `Gender`: NaNs were filled with 'Male' based on initial data assessment.
    * `Product Category`: Missing categories were imputed using `KNNImputer` (with k=3). This involved label encoding the 'Product Category', scaling relevant numeric features, performing imputation, and then inverse-transforming the imputed labels back to their original category names.
    * `Quantity`: NaNs were filled with the median value (3.0). Any negative quantities were also adjusted to 3.
    * `Price per Unit`: NaNs were filled with the mode (50.0). Negative prices were corrected to 20.
    * `Total Amount`: To ensure data integrity, this column was entirely recalculated as `Quantity * Price per Unit` after the imputation of its constituent features.
* **Duplicate Removal:** The dataset was scanned for duplicate entries; none were identified.

### Exploratory Data Analysis (EDA) & Feature Engineering

To prepare the data for clustering and extract further insights:

* **Feature Engineering:**
    * `Gender_Male`: A binary dummy variable was created from the `Gender` column (1 for Male, 0 for Female).
    * `Product Category`: After imputation, this categorical feature was transformed into numerical dummy variables (`Beauty`, `Clothing`, `Electronics`) for use in the clustering model. The encoding used during imputation was (Beauty:0, Clothing:1, Electronics:2).
    * `days_between`: This feature quantifies customer recency, calculated as the number of days between a fixed "today" date ('2024-01-01') and each customer's last purchase date.
    * `Customer ID`: The 'CUST' prefix was stripped, and the IDs were converted to a numeric format.
* **Normalization:** Key numerical features (`Age`, `Quantity`, `Price per Unit`, `Total Amount`, `days_between`) were scaled to a [0, 1] range using `MinMaxScaler` to ensure equal contribution to the clustering algorithm.
* **EDA Highlights:**
    * **Monthly Sales Trends:** Visual analysis revealed a sales peak in May, with other notable periods being February and the lead-up to Christmas. January showed the lowest sales.
    * **Category Performance:** Profitability across different product categories was examined.
    * **Gender Spending:** EDA suggested that female customers tend to spend slightly more on average.
    * **Age-Based Spending:** Spending patterns across different age demographics were explored.

### Modeling: K-Means Clustering

Customer segmentation was achieved using the K-Means algorithm:

* **Algorithm:** K-Means.
* **Optimal Number of Clusters (K):** The Elbow Method was employed to identify the optimal K. The analysis pointed to K=3 as the most suitable number of clusters.
* **Features for Clustering:** The model was trained on the following preprocessed features: `Age`, `Quantity`, `Price per Unit`, `Total Amount`, `Gender_Male`, `Beauty`, `Clothing`, `Electronics`, and `days_between`.
* `Customer ID` and `Transaction ID` were excluded from the feature set for clustering.

### Cluster Interpretation

The K-Means algorithm identified three distinct customer segments:

* **Cluster 0 (Thrifty Clothing Shoppers):** Predominantly purchase clothing. They are characterized by lower spending per transaction, a preference for items with a lower price per unit, and less recent purchase activity.
* **Cluster 1 (Recent Electronics Consumers):** Focus on electronics. Their purchases are more recent, and their transaction values fall into a mid-range.
* **Cluster 2 (Younger Beauty Aficionados):** This segment comprises slightly younger customers, primarily women, who exhibit higher spending on beauty products, particularly those with a higher price per unit.

---

## üõ†Ô∏è Prerequisites

* Python 3.x
* Essential Python libraries (refer to `requirements.txt` for specific versions):
    * `requests`
    * `pandas`
    * `numpy`
    * `seaborn`
    * `matplotlib`
    * `scikit-learn`

---

## ‚öôÔ∏è Setup & Installation

1.  **Clone the Repository:**
    ```bash
    git clone [YOUR_REPOSITORY_URL]
    cd [REPOSITORY_NAME]
    ```
2.  **Create and Activate a Virtual Environment** (Recommended):
    ```bash
    python -m venv venv
    # On macOS/Linux:
    source venv/bin/activate
    # On Windows:
    # venv\Scripts\activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(See below for notes on generating `requirements.txt`)*

---

## üöÄ Usage

1.  **Data Acquisition:**
    Execute the `API.py` script to fetch the transaction data:
    ```bash
    python API.py
    ```
    This will generate the `dataset.csv` file in the project's root directory.

2.  **Clustering Analysis:**
    Open and run the `project.ipynb` Jupyter Notebook.
    * **CRITICAL:** Before execution, ensure the data loading path in the notebook:
        `dataset = pd.read_csv('/Users/liakooras/Desktop/Msc/CAPSTONE/Dataset (1).csv')`
        is updated to the locally generated `dataset.csv` (e.g., `dataset = pd.read_csv('dataset.csv')`).
    * Run all cells sequentially to perform data cleaning, EDA, model training, and view cluster interpretations.

---

## üìä Output

* **`dataset.csv`:** The raw transaction data obtained from the API.
* **Jupyter Notebook (`project.ipynb`):** Contains detailed analysis, including:
    * Data visualizations and EDA insights.
    * Customer segments derived from K-Means clustering.
    * Interpretation of these customer segments.
    * The final DataFrame with cluster labels (`df_for_clustering`) is available within the notebook.

---

## üìù Future Work & Notes

* The notebook indicates that a full RFM (Recency, Frequency, Monetary) analysis was challenging due to the dataset showing a purchase frequency of 1 for all clients after cleaning. Investigating this further with more granular or extensive data could be a valuable next step.


---

### `requirements.txt` Generation

To ensure reproducibility, it's crucial to list the exact versions of the libraries used. After installing all necessary packages in your virtual environment, generate the `requirements.txt` file using:

